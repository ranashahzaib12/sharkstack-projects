# -*- coding: utf-8 -*-
"""StudentBotHelp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iNtn0BaMxCPPColK7492A-l3NL7Igrpz
"""

# Clear Colab environment and install required packages
# !pip uninstall -y pinecone-client groq sentence-transformers pypdf python-docx langchain langchain-community langchain-groq huggingface_hub
# !pip install pinecone-client groq sentence-transformers pypdf python-docx langchain langchain-community langchain-groq --quiet

import os
import io
from google.colab import files
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec
from pypdf import PdfReader
import docx
from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter




#  Set API Keys (Colab way)
os.environ["GROQ_API_KEY"] = " "
os.environ["PINECONE_API_KEY"] = ""
os.environ["HF_TOKEN"] = ""

try:

    if not all([os.getenv("GROQ_API_KEY"), os.getenv("PINECONE_API_KEY")]):
        raise ValueError("Please set GROQ_API_KEY and PINECONE_API_KEY environment variables.")

    # Initialize Clients
    groq_client = ChatGroq(api_key=os.getenv("GROQ_API_KEY"), model="openai/gpt-oss-120b")  # Updated to a valid Groq model
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    print(" Initialized Groq, Pinecone, and SentenceTransformer")
except Exception as e:
    print(f"Error initializing clients: {str(e)}")
    exit(1)


index_name = "study-assistant-rag"
try:
    if index_name not in pc.list_indexes().names():
        pc.create_index(
            name=index_name,
            dimension=384,  # for all-MiniLM-L6-v2 embeddings
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
    index = pc.Index(index_name)
    print(" Connected to Pinecone index")
except Exception as e:
    print(f"Error creating/connecting to Pinecone index: {str(e)}")
    exit(1)


#  Upload & Process Document

def process_file(filename, content):
    try:
        print(f"Processing file: {filename}")
        text = ""
        if filename.endswith(".pdf"):
            reader = PdfReader(io.BytesIO(content))
            for i, page in enumerate(reader.pages):
                extracted = page.extract_text()
                if extracted:
                    text += extracted + " "
                else:
                    print(f"Warning: No text extracted from page {i+1} in {filename}")
            print(f"Extracted text length from PDF: {len(text)} characters")
        elif filename.endswith(".docx"):
            doc = docx.Document(io.BytesIO(content))
            text = " ".join([para.text for para in doc.paragraphs if para.text])
            print(f"Extracted text length from DOCX: {len(text)} characters")
        else:
            raise ValueError(f"Unsupported file format: {filename}")
        return text
    except Exception as e:
        print(f"Error processing file {filename}: {str(e)}")
        return ""

# Upload file or use manual text input
try:
    print("Please upload a PDF or DOCX file with study material...")
    uploaded = files.upload()

    text = ""
    if uploaded:
        for filename, content in uploaded.items():
            extracted_text = process_file(filename, content)
            if extracted_text:
                text += extracted_text
            print(f"Total text length after processing {filename}: {len(text)} characters")
    else:
        print("No file uploaded. Please enter study material text manually:")
        text = input("Enter text: ")
        print(f"Manually entered text length: {len(text)} characters")
except Exception as e:
    print(f"Error during file upload: {str(e)}")
    print("Falling back to manual text input:")
    text = input("Enter text: ")
    print(f"Manually entered text length: {len(text)} characters")


#  Chunking and Embedding

if text.strip():
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
        chunks = text_splitter.split_text(text)
        print(f" Processed {len(chunks)} chunks: {chunks[:2]}...")

        def embed(texts):
            try:
                embeddings = []
                for t in texts:
                    if not t.strip():
                        print("Skipping empty text chunk")
                        continue
                    print(f"Generating embedding for text: {t[:50]}...")
                    emb = embedder.encode(t, convert_to_numpy=True)
                    embeddings.append(emb.tolist())
                    print(f"Embedding generated, length: {len(embeddings[-1])}")
                return embeddings
            except Exception as e:
                print(f"Error generating embeddings: {str(e)}")
                return []

        vectors = embed(chunks)

        if vectors:
            try:
                index.upsert(vectors=[
                    (f"id-{i}", vec, {"text": chunk})
                    for i, (vec, chunk) in enumerate(zip(vectors, chunks))
                    if np.any(vec)
                ])
                print(" Uploaded embeddings to Pinecone.")
            except Exception as e:
                print(f"Error uploading to Pinecone: {str(e)}")
        else:
            print("No valid vectors generated. Check document content.")
    except Exception as e:
        print(f"Error during chunking/embedding: {str(e)}")
else:
    print("No valid text extracted or entered. Using default text for testing.")
    text = "Machine learning is a method of data analysis that automates analytical model building."
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
    chunks = text_splitter.split_text(text)
    vectors = embed(chunks)
    if vectors:
        index.upsert(vectors=[
            (f"id-{i}", vec, {"text": chunk})
            for i, (vec, chunk) in enumerate(zip(vectors, chunks))
            if np.any(vec)
        ])
        print(" Uploaded default text embeddings to Pinecone.")


#  Setup Study Assistant

memory = ConversationBufferMemory(memory_key="chat_history", input_key="topic")

template = """
You are a helpful Study Assistant.
The student wants to learn about: {topic}

Relevant study material:
{context}

Conversation so far:
{chat_history}

Please provide:
1. A short explanation.
2. Key points in bullet form.
3. A motivational closing message.
"""

prompt = PromptTemplate(
    input_variables=["topic", "context", "chat_history"],
    template=template,
)


#  Query Function with RAG

def ask_study_assistant(query):
    try:
        print(f"Processing query: {query}")
        q_emb = embed([query])[0]

        res = index.query(vector=q_emb, top_k=3, include_metadata=True)
        context = "\n".join([m["metadata"]["text"] for m in res["matches"]]) if res["matches"] else "No relevant study material found."
        print(f"Retrieved context: {context[:100]}...")

        response = chain.invoke({"topic": query, "context": context})
        return response["text"]
    except Exception as e:
        print(f"Error processing question: {str(e)}")
        return f"Error: {str(e)}"

# LLM Chain
chain = LLMChain(
    llm=groq_client,
    prompt=prompt,
    memory=memory,
)


#  Interactive Testing

print("Study Assistant Ready! Upload a file or use default text.\n")

while True:
    try:
        user_input = input(" Ask your study question (or type 'exit'): ")
        if user_input.lower() in ["exit", "quit"]:
            print(" Goodbye! Keep learning!")
            break
        answer = ask_study_assistant(user_input)
        print("\n Assistant:\n", answer, "\n")
    except KeyboardInterrupt:
        print("\nExiting...")
        break
    except Exception as e:
        print(f"Error in interactive loop: {str(e)}")